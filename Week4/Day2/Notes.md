# ğŸ“Š Day 2: Data Cleaning & Preprocessing ğŸ§¹âœ¨

---

## ğŸ¯ Introduction: Why Clean Data? ğŸ’­

Imagine you're a chef ğŸ‘¨â€ğŸ³ preparing a meal ğŸ½ï¸. You wouldn't use rotten vegetables ğŸ¥¬âŒ or unwashed ingredients ğŸ§¼â“, right? Similarly, **raw data** ğŸ“Š is messy ğŸŒªï¸, incomplete ğŸ•³ï¸, and inconsistent âš ï¸. **Data cleaning** ğŸ§¹ is the process of preparing your data "ingredients" ğŸ¥˜ before cooking up insights ğŸ’¡âœ¨.

> **Real-life analogy** ğŸŒ: Your contact list ğŸ“± might have duplicate entries ğŸ‘¥ğŸ‘¥, missing phone numbers â˜ï¸â“, or names in different formats (John vs. JOHN ğŸ”¤). Cleaning ensures consistency âœ…!

---

## ğŸ” 1. Identifying & Handling Missing Data ğŸ•³ï¸âŒ

### ğŸ§ What is Missing Data?

**Missing data** = **empty cells** ğŸ“­ in your dataset ğŸ“Š where values should exist âœ….

**Types of Missing Data:** ğŸ—‚ï¸

- âŒ **Completely Missing** â†’ No value at all (`NaN`, `None`, `NULL`)
- âš ï¸ **Placeholder Values** â†’ `-999`, `0`, `"Unknown"` used as fillers ğŸ”¢
- ğŸ­ **Hidden Missing** â†’ Spaces `" "`, empty strings `""`

---

### ğŸ› ï¸ Detection Methods ğŸ”ğŸ”¬

```python
import pandas as pd  # ğŸ“š Pandas library
import numpy as np   # ğŸ”¢ NumPy for numerical operations

# ğŸ“‚ Sample dataset with missing values ğŸ•³ï¸
data = {
    'Name': ['Alice', 'Bob', None, 'David', 'Eve'],          # â“ Missing name
    'Age': [25, np.nan, 35, 40, 28],                         # â“ Missing age  
    'Salary': [50000, 60000, 75000, np.nan, 52000],         # ğŸ’°â“ Missing salary
    'Department': ['HR', 'IT', 'Finance', 'IT', None]        # ğŸ¢â“ Missing dept
}

df = pd.DataFrame(data)  # ğŸ“Š Create DataFrame

# ğŸ” Method 1: Check for missing values (returns True/False) âœ…âŒ
print(df.isnull())  

# ğŸ” Method 2: Count missing values per column ğŸ”¢
print(df.isnull().sum())  # ğŸ“Š Shows: Name(1), Age(1), Salary(1), Department(1)

# ğŸ” Method 3: Percentage of missing data ğŸ“ŠğŸ“ˆ
print((df.isnull().sum() / len(df)) * 100)  # ğŸ¯ Each column has 20% missing

# ğŸ” Method 4: Visual representation ğŸ–¼ï¸
import missingno as msno  # ğŸ“Š Visualization library
msno.matrix(df)  # ğŸ¨ Shows missing data pattern
```

**Linear Explanation** ğŸ“: We use `.isnull()` to detect missing values â“. It returns `True` âœ… for missing cells and `False` âŒ for filled ones. Summing gives us the count ğŸ”¢ of missing values per column ğŸ“Š.

---

### ğŸ¯ Handling Strategies ğŸ› ï¸

#### **Strategy 1ï¸âƒ£: Deletion** ğŸ—‘ï¸âŒ
 
**When to use** â°: When missing data is < 5% ğŸ“‰ of total data.

```python
# ğŸ—‘ï¸ Remove rows with ANY missing value
df_cleaned = df.dropna()  # âŒ Removes all incomplete rows

# ğŸ—‘ï¸ Remove rows where SPECIFIC column is missing
df_cleaned = df.dropna(subset=['Age'])  # âŒ Only removes rows with missing Age

# ğŸ—‘ï¸ Remove columns with missing values ğŸ“ŠâŒ
df_cleaned = df.dropna(axis=1)  # âš ï¸ Removes entire columns (be careful!)

# âš ï¸ Remove rows with more than 50% missing data
threshold = len(df.columns) * 0.5  # ğŸ¯ Calculate threshold
df_cleaned = df.dropna(thresh=threshold)  
```

**âš ï¸ Warning**: Deletion can lose valuable information ğŸ“‰ğŸ’”!

---

#### **Strategy 2ï¸âƒ£: Imputation (Filling)** ğŸ”§âœ¨

**When to use** â°: When missing data is 5-40% ğŸ“Š and you want to preserve data ğŸ’¾.

```python
# ğŸ“ Method 1: Fill with a constant value ğŸ”¢
df['Department'].fillna('Unknown', inplace=True)  # ğŸ¢â“ â†’ ğŸ¢"Unknown"

# ğŸ“Š Method 2: Fill with mean (for numerical data) ğŸ“ˆ
mean_age = df['Age'].mean()  # ğŸ§® Calculate average age
df['Age'].fillna(mean_age, inplace=True)  # ğŸ“Š Replace missing with average

# ğŸ“Š Method 3: Fill with median (better for outliers) ğŸ“‰
median_salary = df['Salary'].median()  # ğŸ¯ Middle value
df['Salary'].fillna(median_salary, inplace=True)  # ğŸ’° Robust to outliers

# ğŸ“Š Method 4: Fill with mode (most frequent value) ğŸ”
mode_dept = df['Department'].mode()[0]  # ğŸ† Most common department
df['Department'].fillna(mode_dept, inplace=True)  

# ğŸ”„ Method 5: Forward Fill (use previous value) â¬‡ï¸
df.fillna(method='ffill', inplace=True)  # ğŸ“‹ Copies value from above

# ğŸ”„ Method 6: Backward Fill (use next value) â¬†ï¸
df.fillna(method='bfill', inplace=True)  # ğŸ“‹ Copies value from below

# ğŸ¤– Method 7: Advanced - Use machine learning prediction ğŸ§ 
from sklearn.impute import KNNImputer  # ğŸ¯ K-Nearest Neighbors

imputer = KNNImputer(n_neighbors=3)  # ğŸ‘¥ Use 3 nearest neighbors
df_imputed = pd.DataFrame(imputer.fit_transform(df.select_dtypes(include=[np.number])))
# ğŸ”¬ Predicts missing values based on similar rows
```

**Real-life example** ğŸŒ: If a student ğŸ‘¨â€ğŸ“ misses one exam ğŸ“â“, you might fill it with their average score ğŸ“Š (mean imputation) rather than failing them âŒ.

---

## ğŸ”„ 2. Handling Duplicate Data ğŸ‘¥ğŸ‘¥

### ğŸ§ What are Duplicates?

**Duplicate rows** = identical entries ğŸ“‹ğŸ“‹ that appear multiple times Ã—2, Ã—3, Ã—4...

```python
# ğŸ“Š Sample data with duplicates ğŸ‘¥
data = {
    'ID': [1, 2, 3, 2, 4, 3],              # ğŸ†” IDs (2 and 3 repeated!)
    'Name': ['Alice', 'Bob', 'Charlie', 'Bob', 'David', 'Charlie'],
    'Score': [85, 90, 78, 90, 92, 78]     # ğŸ“ˆ Scores
}

df = pd.DataFrame(data)

# ğŸ” Check for duplicates (returns True/False) âœ…âŒ
print(df.duplicated())  # ğŸ¯ Shows which rows are duplicates

# ğŸ”¢ Count total duplicates
print(df.duplicated().sum())  # ğŸ“Š Result: 2 duplicates found

# ğŸ” Find duplicate rows (show actual data) ğŸ‘€
print(df[df.duplicated()])  # ğŸ“‹ Displays: Bob's and Charlie's duplicate entries

# ğŸ—‘ï¸ Remove duplicates (keep first occurrence) âœ‚ï¸
df_unique = df.drop_duplicates()  # âœ… Keeps first, removes rest

# ğŸ—‘ï¸ Remove duplicates (keep last occurrence) ğŸ”„
df_unique = df.drop_duplicates(keep='last')  # âœ… Keeps last, removes earlier

# ğŸ¯ Remove duplicates based on specific columns only ğŸ“Š
df_unique = df.drop_duplicates(subset=['Name'])  # ğŸ” Only checks Name column

# âš ï¸ Remove ALL occurrences of duplicates (keep none!) âŒ
df_unique = df.drop_duplicates(keep=False)  
```

**Real-life example** ğŸŒ: Your email inbox ğŸ“§ might receive the same message twice Ã—2. You'd delete one copy ğŸ—‘ï¸ to avoid confusion!

---

## âš–ï¸ 3. Handling Inconsistent Data ğŸ”€âš ï¸

### ğŸ§ What is Inconsistent Data?

**Inconsistent data** = same information ğŸ“Š represented differently ğŸ”„.

**Examples** ğŸ“:

- ğŸ‡ºğŸ‡¸ "USA" vs "United States" vs "US" vs "U.S.A"
- ğŸ“… "01/12/2023" vs "2023-12-01" vs "Dec 1, 2023"
- ğŸ”¤ "john" vs "John" vs "JOHN" vs " John "

```python
# ğŸ“Š Sample inconsistent data âš ï¸
data = {
    'Country': ['USA', 'usa', 'U.S.A', 'United States', 'US'],  # ğŸ‡ºğŸ‡¸ Different formats
    'Date': ['01-12-2023', '2023/12/01', 'Dec 1 2023', '1/12/23', '2023-12-01'],  # ğŸ“…
    'Price': ['$100', '100 USD', '100', '$100.00', '100.0']     # ğŸ’° Different formats
}

df = pd.DataFrame(data)

# ğŸ”§ Solution 1: Standardize text (lowercase) ğŸ”¤â¬‡ï¸
df['Country'] = df['Country'].str.lower()  # ğŸ‡ºğŸ‡¸ All become "usa"

# ğŸ”§ Solution 2: Replace variations with standard value ğŸ”„
country_mapping = {
    'usa': 'United States',
    'u.s.a': 'United States',
    'us': 'United States',
    'united states': 'United States'
}
df['Country'] = df['Country'].str.lower().replace(country_mapping)  # âœ… All â†’ United States

# ğŸ”§ Solution 3: Remove whitespace âœ‚ï¸
df['Country'] = df['Country'].str.strip()  # ğŸ§¹ "  USA  " â†’ "USA"

# ğŸ“… Solution 4: Standardize dates â°
df['Date'] = pd.to_datetime(df['Date'])  # ğŸ”„ Converts all to datetime format

# ğŸ’° Solution 5: Clean currency (remove symbols $) ğŸ’µ
df['Price'] = df['Price'].str.replace('$', '').str.replace('USD', '').str.strip()
df['Price'] = pd.to_numeric(df['Price'])  # ğŸ”¢ Convert to numbers

# ğŸ”¤ Solution 6: Capitalize consistently (Title Case) âœ¨
df['Name'] = df['Name'].str.title()  # "john" â†’ "John", "JOHN" â†’ "John"
```

**Linear Explanation** ğŸ“: We use string methods ğŸ”¤ (`.str.lower()`, `.str.replace()`) to standardize text âœ…. This ensures "USA" ğŸ‡ºğŸ‡¸, "usa", and "U.S.A" all become one consistent value ğŸ¯.

---

## ğŸ“ 4. Data Transformation: Scaling ğŸ“Šâš–ï¸

### ğŸ§ Why Scale Data?

**Scaling** ğŸ“ brings different features ğŸ“Š to the same range ğŸ¯, preventing large values ğŸ“ˆ from dominating small ones ğŸ“‰.

**Real-life example** ğŸŒ: Comparing salaries ğŸ’° ($50,000-$200,000) with ages ğŸ‘¶ğŸ‘´ (20-65). Without scaling, salary dominates! âš–ï¸âŒ

---

### ğŸ› ï¸ Scaling Techniques ğŸ”§

#### **Technique 1ï¸âƒ£: Normalization (Min-Max Scaling)** ğŸ“Š

**Formula** ğŸ“: `(X - X_min) / (X_max - X_min)` â†’ Scales to [0, 1] ğŸ¯

```python
from sklearn.preprocessing import MinMaxScaler  # ğŸ“¦ Import scaler

# ğŸ“Š Sample data
data = {
    'Age': [25, 35, 45, 55, 65],           # ğŸ‘¶ğŸ‘´ Range: 25-65
    'Salary': [50000, 75000, 100000, 150000, 200000]  # ğŸ’° Range: 50k-200k
}

df = pd.DataFrame(data)

# ğŸ”§ Apply Min-Max Scaling âš–ï¸
scaler = MinMaxScaler()  # ğŸ› ï¸ Create scaler object
df_scaled = pd.DataFrame(
    scaler.fit_transform(df),  # ğŸ”„ Transform data to [0,1]
    columns=df.columns
)

print(df_scaled)  # âœ… All values now between 0 and 1
# Age: 25â†’0.0, 65â†’1.0 | Salary: 50kâ†’0.0, 200kâ†’1.0
```

**When to use** â°: Neural networks ğŸ§ , image processing ğŸ–¼ï¸, when you need [0,1] range ğŸ“Š.

---

#### **Technique 2ï¸âƒ£: Standardization (Z-Score Scaling)** ğŸ“ˆ

**Formula** ğŸ“: `(X - mean) / std_dev` â†’ Mean=0 ğŸ¯, Std=1 ğŸ“Š

```python
from sklearn.preprocessing import StandardScaler  # ğŸ“¦ Import scaler

# ğŸ”§ Apply Standardization ğŸ“Š
scaler = StandardScaler()  # ğŸ› ï¸ Create scaler object
df_standardized = pd.DataFrame(
    scaler.fit_transform(df),  # ğŸ”„ Transform: mean=0, std=1
    columns=df.columns
)

print(df_standardized)  # âœ… Mean â‰ˆ 0, Std â‰ˆ 1
# Values can be negative â– or positive â•
```

**When to use** â°: Machine learning algorithms ğŸ¤– (SVM, KNN, Linear Regression), when features have different units ğŸ“.

---

#### **Technique 3ï¸âƒ£: Robust Scaling** ğŸ’ª

Uses **median** ğŸ“Š and **IQR** ğŸ“‰ (Interquartile Range) â†’ Robust to outliers ğŸ¯!

```python
from sklearn.preprocessing import RobustScaler  # ğŸ“¦ Import robust scaler

# ğŸ”§ Apply Robust Scaling (handles outliers better) ğŸ’ª
scaler = RobustScaler()  # ğŸ› ï¸ Create scaler
df_robust = pd.DataFrame(
    scaler.fit_transform(df),  # ğŸ”„ Uses median & IQR
    columns=df.columns
)

# âœ… Works well even with extreme values ğŸ“ˆğŸ“‰!
```

**When to use** â°: When data has **outliers** âš¡ (extreme values that would distort mean/std).

---

## ğŸ·ï¸ 5. Encoding Categorical Variables ğŸ”¤â¡ï¸ğŸ”¢

### ğŸ§ What is Encoding?

**Encoding** ğŸ”„ converts **text categories** ğŸ”¤ (like "Red" ğŸ”´, "Blue" ğŸ”µ) into **numbers** ğŸ”¢ that machines ğŸ¤– can understand.

**Why needed?** ğŸ¤”: Machine learning algorithms ğŸ§  only understand numbers ğŸ”¢, not text ğŸ”¤!

---

### ğŸ› ï¸ Encoding Techniques ğŸ·ï¸

#### **Technique 1ï¸âƒ£: Label Encoding** ğŸ·ï¸ğŸ”¢

Assigns each category a **unique number** ğŸ”¢ (0, 1, 2, 3...).

```python
from sklearn.preprocessing import LabelEncoder  # ğŸ“¦ Import encoder

# ğŸ“Š Sample data with categories ğŸ·ï¸
data = {
    'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red', 'Green'],  # ğŸ¨ Colors
    'Size': ['S', 'M', 'L', 'M', 'S', 'L']                      # ğŸ“ Sizes
}

df = pd.DataFrame(data)

# ğŸ”§ Apply Label Encoding ğŸ”¢
encoder = LabelEncoder()  # ğŸ› ï¸ Create encoder
df['Color_Encoded'] = encoder.fit_transform(df['Color'])
# ğŸ”´Redâ†’0, ğŸ”µBlueâ†’1, ğŸŸ¢Greenâ†’2

df['Size_Encoded'] = encoder.fit_transform(df['Size'])
# Sâ†’0, Mâ†’1, Lâ†’2

print(df)  # âœ… New columns with numeric codes
```

**âš ï¸ Warning**: Creates **ordinal relationship** ğŸ“Š (0 < 1 < 2). Use only when order matters ğŸ“ˆ!

**When to use** â°: Tree-based models ğŸŒ² (Decision Trees, Random Forest), ordinal categories ğŸ“Š (Low < Medium < High).

---

#### **Technique 2ï¸âƒ£: One-Hot Encoding** ğŸ”¥1ï¸âƒ£0ï¸âƒ£

Creates **binary columns** 1ï¸âƒ£0ï¸âƒ£ for each category (1=Yes âœ…, 0=No âŒ).

```python
# ğŸ”§ Apply One-Hot Encoding ğŸ”¥
df_encoded = pd.get_dummies(df, columns=['Color', 'Size'])
# ğŸ¨ Creates: Color_Red, Color_Blue, Color_Green
# ğŸ“ Creates: Size_S, Size_M, Size_L

print(df_encoded)  
# âœ… Each category becomes a separate column (0 or 1)
# Example: Red â†’ Color_Red=1, Color_Blue=0, Color_Green=0

# ğŸ”§ Alternative: Using sklearn ğŸ“¦
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse=False)  # ğŸ› ï¸ sparse=False gives array
encoded_array = encoder.fit_transform(df[['Color', 'Size']])
# ğŸ”„ Returns numpy array with binary columns
```

**When to use** â°: **Nominal categories** ğŸ·ï¸ (no order: Red, Blue, Green), linear models ğŸ“ˆ (Logistic Regression), neural networks ğŸ§ .

---

#### **Technique 3ï¸âƒ£: Binary Encoding** 0ï¸âƒ£1ï¸âƒ£

Converts categories to **binary numbers** ğŸ’» (0s and 1s) â†’ Saves space ğŸ’¾!

```python
import category_encoders as ce  # ğŸ“¦ Import category encoders library

# ğŸ”§ Apply Binary Encoding 0ï¸âƒ£1ï¸âƒ£
encoder = ce.BinaryEncoder(cols=['Color'])  # ğŸ› ï¸ Specify columns
df_binary = encoder.fit_transform(df)

# ğŸ¨ Redâ†’001, Blueâ†’010, Greenâ†’011 (in binary columns)
# âœ… Uses fewer columns than One-Hot! ğŸ“ŠğŸ’¾
```

**When to use** â°: Many categories ğŸ“š (100+ levels), memory constraints ğŸ’¾.

---

#### **Technique 4ï¸âƒ£: Target/Mean Encoding** ğŸ¯ğŸ“Š

Replaces category with **average target value** ğŸ“ˆ for that category.

```python
# ğŸ“Š Example: Customer satisfaction by product ğŸ›ï¸
data = {
    'Product': ['Phone', 'Laptop', 'Tablet', 'Phone', 'Laptop', 'Tablet'],  # ğŸ“±ğŸ’»
    'Satisfaction': [4.5, 4.8, 3.9, 4.3, 4.9, 4.0]  # â­ Rating
}

df = pd.DataFrame(data)

# ğŸ”§ Calculate mean satisfaction per product ğŸ“Š
target_encoding = df.groupby('Product')['Satisfaction'].mean()
# ğŸ“±Phone â†’ 4.4, ğŸ’»Laptop â†’ 4.85, Tablet â†’ 3.95

# ğŸ”„ Replace categories with their mean values ğŸ¯
df['Product_Encoded'] = df['Product'].map(target_encoding)

print(df)  # âœ… Products replaced with average satisfaction
```

**âš ï¸ Warning**: Risk of **data leakage** ğŸ’§! Use carefully with cross-validation ğŸ”„.

**When to use** â°: High-cardinality categories ğŸ“š (many unique values), tree-based models ğŸŒ².

---

## ğŸ¯ 6. Outlier Detection ğŸ“Šâš¡

### ğŸ§ What are Outliers?

**Outliers** âš¡ = extreme values ğŸ“ˆğŸ“‰ that are **far away** ğŸš€ from other data points ğŸ“Š.

**Real-life example** ğŸŒ: In a class of students ğŸ‘¨â€ğŸ“ earning $30k-$60k internships ğŸ’°, one gets $500k ğŸ’¸! That's an outlier âš¡!

---

### ğŸ› ï¸ Detection Methods ğŸ”

#### **Method 1ï¸âƒ£: IQR (Interquartile Range)** ğŸ“ŠğŸ“¦

**IQR** = Q3 - Q1 (difference between 75th and 25th percentiles ğŸ“Š)

**Outliers** âš¡ = Values **below Q1 - 1.5Ã—IQR** â¬‡ï¸ or **above Q3 + 1.5Ã—IQR** â¬†ï¸

```python
import numpy as np
import pandas as pd

# ğŸ“Š Sample data with outliers âš¡
data = {
    'Salary': [50000, 55000, 52000, 60000, 58000, 300000, 54000, 57000]
    # ğŸ’° Notice: 300000 is way higher! âš¡
}

df = pd.DataFrame(data)

# ğŸ”¢ Calculate Q1, Q3, and IQR ğŸ“Š
Q1 = df['Salary'].quantile(0.25)  # ğŸ“‰ 25th percentile
Q3 = df['Salary'].quantile(0.75)  # ğŸ“ˆ 75th percentile
IQR = Q3 - Q1  # ğŸ“¦ Interquartile Range

# ğŸ¯ Define outlier boundaries ğŸš§
lower_bound = Q1 - 1.5 * IQR  # â¬‡ï¸ Lower fence
upper_bound = Q3 + 1.5 * IQR  # â¬†ï¸ Upper fence

# ğŸ” Identify outliers âš¡
outliers = df[(df['Salary'] < lower_bound) | (df['Salary'] > upper_bound)]
print(f"Outliers found âš¡: \n{outliers}")  # ğŸ’° 300000 detected!

# ğŸ—‘ï¸ Remove outliers âœ‚ï¸
df_clean = df[(df['Salary'] >= lower_bound) & (df['Salary'] <= upper_bound)]
print(f"Clean data âœ…: \n{df_clean}")

# ğŸ“Š Visual check: Boxplot ğŸ“¦
import matplotlib.pyplot as plt
df.boxplot(column='Salary')  # ğŸ“ˆ Shows outliers as dots ğŸ”´
plt.show()
```

**Linear Explanation** ğŸ“: IQR method looks at the "middle 50%" ğŸ“¦ of data. Anything **1.5Ã— this range** away from the box edges ğŸ“Š is considered unusual âš¡.

---

#### **Method 2ï¸âƒ£: Z-Score** ğŸ“ŠğŸ“ˆ

**Z-Score** = How many **standard deviations** ğŸ“ away from the mean ğŸ“Š?

**Formula** ğŸ“: `Z = (X - mean) / std_dev`

**Outliers** âš¡ = |Z-score| > 3 (more than 3 standard deviations away ğŸš€)

```python
from scipy import stats  # ğŸ“¦ Statistical functions

# ğŸ“Š Calculate Z-scores ğŸ”¢
df['Z_Score'] = stats.zscore(df['Salary'])  # ğŸ§® Compute Z-score

# ğŸ” Identify outliers (|Z| > 3) âš¡
outliers = df[np.abs(df['Z_Score']) > 3]
print(f"Outliers (Z-score) âš¡: \n{outliers}")  # ğŸ’° 300000 detected!

# ğŸ—‘ï¸ Remove outliers âœ‚ï¸
df_clean = df[np.abs(df['Z_Score']) <= 3]
print(f"Clean data âœ…: \n{df_clean}")

# ğŸ“Š Interpretation:
# Z = 0 â†’ At mean ğŸ¯
# Z = 1 â†’ 1 std dev above mean â¬†ï¸
# Z = -1 â†’ 1 std dev below mean â¬‡ï¸
# Z > 3 â†’ Extremely high âš¡ğŸ“ˆ
# Z < -3 â†’ Extremely low âš¡ğŸ“‰
```

**When to use** â°: **Normally distributed** ğŸ”” data (bell curve ğŸ“Š), when you want strict thresholds ğŸ¯.

**âš ï¸ Limitation**: Doesn't work well with **skewed data** â†—ï¸ (e.g., income ğŸ’°).

---

#### **Method 3ï¸âƒ£: Isolation Forest** ğŸŒ²ğŸ¤–

**Machine learning approach** ğŸ§  that isolates anomalies âš¡ by building random trees ğŸŒ².

```python
from sklearn.ensemble import IsolationForest  # ğŸ“¦ ML-based outlier detection

# ğŸ”§ Apply Isolation Forest ğŸŒ²
model = IsolationForest(contamination=0.1)  # ğŸ¯ Expect 10% outliers
# contamination = expected proportion of outliers ğŸ“Š

df['Outlier'] = model.fit_predict(df[['Salary']])
# ğŸ” Returns: -1 (outlier âš¡), 1 (normal âœ…)

# ğŸ—‘ï¸ Filter outliers âœ‚ï¸
outliers = df[df['Outlier'] == -1]
df_clean = df[df['Outlier'] == 1]

print(f"Outliers detected âš¡: \n{outliers}")
print(f"Clean data âœ…: \n{df_clean}")
```

**When to use** â°: **Complex data** ğŸ§©, **multivariate outliers** ğŸ“Š (outliers across multiple columns), when IQR/Z-score fail âŒ.

---

### ğŸ› ï¸ Handling Outliers Strategy ğŸ¯

Once detected âš¡, you can:

1. **ğŸ—‘ï¸ Remove** â†’ Delete outlier rows âœ‚ï¸ (if they're errors âŒ)
2. **ğŸ”§ Cap/Floor** â†’ Replace with boundary values ğŸš§
3. **ğŸ”„ Transform** â†’ Log/Square root transformation ğŸ“
4. **ğŸ¤” Investigate** â†’ Understand why they exist ğŸ”

```python
# ğŸ”§ Strategy: Capping (Winsorization) ğŸš§
# Replace outliers with boundary values (not delete!) ğŸ“Œ

upper_limit = df['Salary'].quantile(0.95)  # ğŸ“ˆ 95th percentile
lower_limit = df['Salary'].quantile(0.05)  # ğŸ“‰ 5th percentile

# ğŸ”„ Cap extreme values ğŸ¯
df['Salary_Capped'] = df['Salary'].clip(lower=lower_limit, upper=upper_limit)
# âœ… 300000 â†’ becomes 95th percentile value (e.g., 70000)

# ğŸ”„ Strategy: Log Transformation ğŸ“
df['Salary_Log'] = np.log(df['Salary'])  # ğŸ”¢ Reduces impact of outliers
# ğŸ“Š Makes skewed data more normal ğŸ””
```

---

## âš™ï¸ 7. Feature Engineering: Creating Derived Features ğŸ› ï¸âœ¨

### ğŸ§ What is Feature Engineering?

**Feature engineering** âš™ï¸ = Creating **new features** ğŸ†• from existing data ğŸ“Š to improve model performance ğŸ“ˆğŸ¤–.

**Real-life example** ğŸŒ: From "Date of Birth" ğŸ“…, create "Age" ğŸ‘¶ğŸ‘´. From "Height" ğŸ“ & "Weight" âš–ï¸, create "BMI" ğŸ’ª!

---

### ğŸ› ï¸ Common Feature Engineering Techniques ğŸ”§

#### **Technique 1ï¸âƒ£: Mathematical Operations** ğŸ§®â•â–âœ–ï¸â—

```python
# ğŸ“Š Sample data
data = {
    'Length': [10, 15, 20, 25],      # ğŸ“ Length in meters
    'Width': [5, 7, 10, 12],         # ğŸ“ Width in meters
    'Price': [1000, 1500, 2000, 2500],  # ğŸ’° Price
    'Units_Sold': [50, 30, 40, 60]   # ğŸ“¦ Units
}

df = pd.DataFrame(data)

# â• Addition: Total Dimensions ğŸ“Š
df['Total_Dimension'] = df['Length'] + df['Width']  # ğŸ“+ğŸ“

# âœ–ï¸ Multiplication: Area ğŸ 
df['Area'] = df['Length'] * df['Width']  # ğŸ“Ã—ğŸ“ = Area

# â— Division: Price per Unit ğŸ’°
df['Price_Per_Unit'] = df['Price'] / df['Units_Sold']  # ğŸ’°Ã·ğŸ“¦

# ğŸ”¢ Power: Volume (if we had height) ğŸ“¦
# df['Volume'] = df['Length'] * df['Width'] * df['Height']

print(df)  # âœ… New features created!
```

---

