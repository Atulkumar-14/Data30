# ğŸ“š Day 13: Introduction to Other Important Libraries ğŸ”¬

---

## ğŸ¯ Topics to Cover

|ğŸ“Œ Library|ğŸ” Focus Area|
|---|---|
|**ğŸ”¢ NumPy Advanced**|Broadcasting ğŸ“¡, Vectorization âš¡, Matrix Operations ğŸ§®|
|**ğŸ”¬ SciPy**|Statistical Functions ğŸ“Š & Scientific Computations ğŸ§ª|
|**ğŸ“ˆ Statsmodels**|Statistical Modeling ğŸ“‰ & Analysis|
|**ğŸ¤– Sklearn Basics**|Datasets ğŸ“¦ & Preprocessing ğŸ› ï¸ Modules|

---

## ğŸ”¢ 1. NumPy Advanced â€“ Broadcasting ğŸ“¡, Vectorization âš¡, Matrix Operations ğŸ§®

**Goal:** Master advanced NumPy techniques for fast numerical computing.

### ğŸ“¡ Broadcasting (Operating on Different Shapes)

Broadcasting allows NumPy to perform operations on arrays of different shapes without copying data.

```python
import numpy as np  # ğŸ”¢ Import NumPy

# ğŸ“¡ Example 1: Add scalar to array
array = np.array([1, 2, 3, 4, 5])
result = array + 10  # â• Adds 10 to each element
print(result)  # Output: [11 12 13 14 15]

# ğŸ“¡ Example 2: Add 1D array to 2D array
matrix = np.array([[1, 2, 3],
                   [4, 5, 6],
                   [7, 8, 9]])
row = np.array([10, 20, 30])
result = matrix + row  # â• Adds row to each row of matrix
print(result)
# Output:
# [[11 22 33]
#  [14 25 36]
#  [17 28 39]]

# ğŸ“¡ Example 3: Column broadcasting
column = np.array([[10], [20], [30]])  # ğŸ“Š 3x1 column
result = matrix + column  # â• Adds column to each column of matrix
print(result)

# ğŸ“¡ Broadcasting rules:
# âœ… Dimensions must be compatible (equal or one of them is 1)
# âœ… NumPy stretches the smaller array automatically
```

**ğŸ’¡ Linear Explanation:** Broadcasting lets you perform operations between arrays of different sizes without manually copying data. NumPy automatically "stretches" smaller arrays to match larger ones, making operations fast and memory-efficient.

### âš¡ Vectorization (Avoiding Loops for Speed)

Vectorization replaces slow Python loops with fast NumPy operations.

```python
# ğŸŒ Slow way: Using Python loops
numbers = [1, 2, 3, 4, 5]
squared = []
for num in numbers:
    squared.append(num ** 2)  # â° Slow iteration

# âš¡ Fast way: Using vectorization
numbers = np.array([1, 2, 3, 4, 5])
squared = numbers ** 2  # ğŸš€ 100x faster!
print(squared)  # Output: [ 1  4  9 16 25]

# âš¡ Example: Mathematical operations
array = np.array([1, 2, 3, 4, 5])
result = np.sqrt(array) * 2 + 5  # ğŸ§® Multiple operations at once
print(result)

# âš¡ Example: Conditional operations
data = np.array([10, 25, 30, 15, 40])
result = np.where(data > 20, "High", "Low")  # ğŸ” Vectorized condition
print(result)  # Output: ['Low' 'High' 'High' 'Low' 'High']

# âš¡ Example: Apply custom function
def custom_func(x):
    return x ** 2 + 2 * x + 1  # ğŸ“ Formula

vectorized_func = np.vectorize(custom_func)  # ğŸ”„ Make it vectorized
result = vectorized_func(array)
print(result)
```

**ğŸ’¡ Linear Explanation:** Vectorization means doing operations on entire arrays at once instead of looping through elements one by one. This makes code run 10-100x faster because NumPy uses optimized C code underneath.

### ğŸ§® Matrix Operations (Linear Algebra)

```python
# ğŸ§® Create matrices
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# â• Matrix addition
result = A + B  # Element-wise addition
print("Addition â•:\n", result)

# â– Matrix subtraction
result = A - B  # Element-wise subtraction
print("Subtraction â–:\n", result)

# âœ–ï¸ Element-wise multiplication
result = A * B  # Multiply corresponding elements
print("Element-wise âœ–ï¸:\n", result)

# ğŸ”¢ Matrix multiplication (Dot product)
result = np.dot(A, B)  # Or A @ B
print("Matrix Multiplication ğŸ”¢:\n", result)

# ğŸ”„ Transpose (flip rows and columns)
result = A.T  # Or np.transpose(A)
print("Transpose ğŸ”„:\n", result)

# ğŸ” Inverse (Aâ»Â¹)
result = np.linalg.inv(A)
print("Inverse ğŸ”:\n", result)

# ğŸ“Š Determinant
result = np.linalg.det(A)
print("Determinant ğŸ“Š:", result)

# ğŸ¯ Eigenvalues and Eigenvectors
eigenvalues, eigenvectors = np.linalg.eig(A)
print("Eigenvalues ğŸ¯:", eigenvalues)
print("Eigenvectors ğŸ¯:\n", eigenvectors)

# ğŸ“ Solve linear equations: Ax = b
b = np.array([1, 2])
x = np.linalg.solve(A, b)  # Solves for x
print("Solution ğŸ“:", x)

# ğŸ”¢ Matrix rank
rank = np.linalg.matrix_rank(A)
print("Rank ğŸ”¢:", rank)
```

**ğŸ’¡ Linear Explanation:** Matrix operations are essential for scientific computing and machine learning. NumPy provides functions for addition, multiplication, transpose, inverse, and solving equations - all optimized for speed.

---

## ğŸ”¬ 2. SciPy â€“ Statistical Functions ğŸ“Š & Scientific Computations ğŸ§ª

**Goal:** Use SciPy for advanced scientific and statistical computing.

### ğŸ“Š Statistical Functions

```python
from scipy import stats  # ğŸ“Š Import statistics module

# ğŸ“Š Create sample data
data = np.array([10, 12, 23, 23, 16, 23, 21, 16])

# ğŸ“ˆ Descriptive statistics
print("Mean ğŸ“ˆ:", np.mean(data))
print("Median ğŸ“Š:", np.median(data))
print("Mode ğŸ”:", stats.mode(data))
print("Standard Deviation ğŸ“:", np.std(data))
print("Variance ğŸ“Š:", np.var(data))

# ğŸ“Š Probability distributions
# ğŸ”” Normal distribution
mean, std = 0, 1
normal_dist = stats.norm(loc=mean, scale=std)
print("PDF at x=0 ğŸ””:", normal_dist.pdf(0))  # Probability Density Function
print("CDF at x=0 ğŸ“ˆ:", normal_dist.cdf(0))  # Cumulative Distribution Function

# ğŸ² Generate random samples from normal distribution
samples = normal_dist.rvs(size=100)  # ğŸ² 100 random samples
print("Random samples ğŸ²:", samples[:5])

# ğŸ“Š Other distributions
uniform_dist = stats.uniform(loc=0, scale=10)  # ğŸ“ Uniform distribution
binomial_dist = stats.binom(n=10, p=0.5)  # ğŸ¯ Binomial distribution
poisson_dist = stats.poisson(mu=3)  # ğŸ“Š Poisson distribution

# ğŸ§ª Hypothesis testing
# T-test (compare two samples)
sample1 = np.array([10, 12, 14, 16, 18])
sample2 = np.array([11, 13, 15, 17, 19])
t_stat, p_value = stats.ttest_ind(sample1, sample2)  # ğŸ§ª Independent t-test
print(f"T-statistic ğŸ§ª: {t_stat:.4f}, P-value ğŸ“Š: {p_value:.4f}")

# ğŸ”— Correlation test
x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 5, 4, 5])
correlation, p_value = stats.pearsonr(x, y)  # ğŸ”— Pearson correlation
print(f"Correlation ğŸ”—: {correlation:.4f}, P-value ğŸ“Š: {p_value:.4f}")
```

### ğŸ§ª Scientific Computations

```python
from scipy import optimize, integrate, interpolate  # ğŸ§ª Scientific tools

# ğŸ“ Optimization (Finding minimum/maximum)
def objective_function(x):
    return x**2 + 5*x + 6  # ğŸ“ Function to minimize

result = optimize.minimize(objective_function, x0=0)  # ğŸ¯ Start from x=0
print("Minimum value ğŸ¯:", result.fun)
print("At position ğŸ“:", result.x)

# ğŸ” Root finding (solve equations)
def equation(x):
    return x**3 - 2*x - 5  # ğŸ” Find where this equals zero

root = optimize.fsolve(equation, x0=2)  # ğŸ¯ Start from x=2
print("Root ğŸ¯:", root)

# âˆ« Integration (area under curve)
def integrand(x):
    return x**2  # âˆ« Integrate xÂ²

result, error = integrate.quad(integrand, 0, 1)  # âˆ«â‚€Â¹ xÂ² dx
print(f"Integral âˆ«: {result:.4f}")

# ğŸ“ˆ Interpolation (fill gaps in data)
x_points = np.array([0, 1, 2, 3, 4])
y_points = np.array([0, 1, 4, 9, 16])
interpolator = interpolate.interp1d(x_points, y_points, kind='cubic')  # ğŸ“ˆ Smooth curve

x_new = np.linspace(0, 4, 20)  # ğŸ“Š More points
y_new = interpolator(x_new)  # ğŸ“ˆ Interpolated values
print("Interpolated values ğŸ“ˆ:", y_new[:5])

# ğŸŒŠ Signal processing
from scipy import signal
t = np.linspace(0, 1, 1000)
noisy_signal = np.sin(2 * np.pi * 5 * t) + np.random.normal(0, 0.1, 1000)  # ğŸŒŠ Noisy sine wave
filtered = signal.medfilt(noisy_signal, kernel_size=5)  # ğŸ”§ Median filter
print("Signal filtered ğŸ”§ âœ…")
```

**ğŸ’¡ Linear Explanation:** SciPy builds on NumPy to provide advanced scientific tools. It includes statistical tests (t-tests, correlation), optimization (finding min/max), integration (calculating areas), and signal processing - all essential for scientific research.

---

## ğŸ“ˆ 3. Statsmodels â€“ Statistical Modeling ğŸ“‰ & Analysis

**Goal:** Build and analyze statistical models for data relationships.

### ğŸ“‰ Linear Regression (Predicting Values)

```python
import statsmodels.api as sm  # ğŸ“ˆ Import statsmodels
import pandas as pd

# ğŸ“Š Create sample data
data = pd.DataFrame({
    'hours_studied': [1, 2, 3, 4, 5, 6, 7, 8],  # ğŸ“š Independent variable (X)
    'test_score': [50, 55, 60, 65, 70, 75, 80, 85]  # ğŸ¯ Dependent variable (Y)
})

# ğŸ”¢ Prepare variables
X = data['hours_studied']  # ğŸ“Š Predictor
y = data['test_score']  # ğŸ¯ Target
X = sm.add_constant(X)  # â• Add intercept (y = a + bX)

# ğŸ—ï¸ Build the model
model = sm.OLS(y, X)  # ğŸ“ Ordinary Least Squares
results = model.fit()  # ğŸ”§ Fit the model

# ğŸ“Š View summary statistics
print(results.summary())
# Shows: RÂ², coefficients, p-values, confidence intervals

# ğŸ¯ Make predictions
new_hours = sm.add_constant([9, 10])  # ğŸ“š New data points
predictions = results.predict(new_hours)  # ğŸ”® Predict scores
print("Predictions ğŸ”®:", predictions)

# ğŸ“Š Extract key metrics
print("R-squared ğŸ“Š:", results.rsquared)  # How well model fits (0-1)
print("Coefficients ğŸ”¢:", results.params)  # Slope and intercept
print("P-values ğŸ“Š:", results.pvalues)  # Statistical significance
```

### ğŸ“Š Time Series Analysis

```python
from statsmodels.tsa.seasonal import seasonal_decompose  # ğŸ“ˆ Time series tools
from statsmodels.tsa.stattools import adfuller  # ğŸ§ª Statistical tests

# ğŸ“… Create time series data
dates = pd.date_range('2023-01-01', periods=100, freq='D')
values = np.sin(np.linspace(0, 10, 100)) + np.random.normal(0, 0.1, 100)  # ğŸŒŠ Seasonal pattern
ts_data = pd.Series(values, index=dates)

# ğŸ” Decompose time series (Trend + Seasonal + Residual)
decomposition = seasonal_decompose(ts_data, model='additive', period=10)
trend = decomposition.trend  # ğŸ“ˆ Long-term pattern
seasonal = decomposition.seasonal  # ğŸ”„ Repeating pattern
residual = decomposition.resid  # ğŸ“Š Random noise

print("Trend component ğŸ“ˆ extracted âœ…")
print("Seasonal component ğŸ”„ extracted âœ…")

# ğŸ§ª Test for stationarity (constant mean/variance over time)
result = adfuller(ts_data.dropna())
print(f"ADF Statistic ğŸ§ª: {result[0]:.4f}")
print(f"P-value ğŸ“Š: {result[1]:.4f}")
# If p-value < 0.05 âœ… â†’ Data is stationary
# If p-value > 0.05 âŒ â†’ Data is non-stationary
```

### ğŸ“Š Logistic Regression (Classification)

```python
# ğŸ“Š Binary classification example
data = pd.DataFrame({
    'age': [22, 25, 47, 52, 23, 35, 40, 60],  # ğŸ‘¤ Age
    'purchased': [0, 0, 1, 1, 0, 0, 1, 1]  # ğŸ›’ Did they buy? (0=No, 1=Yes)
})

# ğŸ”¢ Prepare variables
X = sm.add_constant(data['age'])  # ğŸ“Š Predictor
y = data['purchased']  # ğŸ¯ Target

# ğŸ—ï¸ Build logistic regression model
logit_model = sm.Logit(y, X)
logit_results = logit_model.fit()  # ğŸ”§ Fit model

# ğŸ“Š View results
print(logit_results.summary())

# ğŸ”® Predict probability of purchase
new_age = sm.add_constant([30, 45, 55])
probabilities = logit_results.predict(new_age)
print("Purchase probabilities ğŸ”®:", probabilities)
# Values between 0-1 (0% to 100% chance)
```

**ğŸ’¡ Linear Explanation:** Statsmodels helps you build statistical models to understand relationships in data. Linear regression predicts continuous values, logistic regression predicts categories (yes/no), and time series analysis finds patterns over time.

---

## ğŸ¤– 4. Sklearn Basics â€“ Datasets ğŸ“¦ & Preprocessing ğŸ› ï¸

**Goal:** Introduction to machine learning with built-in datasets and data preparation.

### ğŸ“¦ Loading Built-in Datasets

```python
from sklearn import datasets  # ğŸ“¦ Import datasets module
import pandas as pd

# ğŸŒ¸ Load Iris dataset (flower classification)
iris = datasets.load_iris()
print("Iris dataset ğŸŒ¸ loaded")
print("Features ğŸ“Š:", iris.feature_names)
print("Target classes ğŸ¯:", iris.target_names)
print("Shape ğŸ“:", iris.data.shape)

# ğŸ¼ Convert to DataFrame
iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)
iris_df['species'] = iris.target
print(iris_df.head())

# ğŸ  Load Boston Housing dataset (price prediction)
# Note: This dataset is deprecated, using California Housing instead
california = datasets.fetch_california_housing()
print("California Housing ğŸ  loaded")

# ğŸ· Load Wine dataset (wine classification)
wine = datasets.load_wine()
print("Wine dataset ğŸ· loaded")

# ğŸ”¢ Load Digits dataset (handwritten numbers)
digits = datasets.load_digits()
print("Digits dataset ğŸ”¢ loaded")
print("Images shape ğŸ“Š:", digits.images.shape)  # 1797 images, 8x8 pixels

# ğŸ² Generate synthetic dataset
X, y = datasets.make_classification(
    n_samples=100,  # ğŸ“Š 100 samples
    n_features=4,  # ğŸ”¢ 4 features
    n_classes=2,  # ğŸ¯ 2 classes
    random_state=42  # ğŸ² Reproducible
)
print("Synthetic dataset ğŸ² created")
```

### ğŸ› ï¸ Data Preprocessing (Preparing Data for ML)

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

# ğŸ“Š Sample data
data = pd.DataFrame({
    'age': [25, 30, 35, 40, 45, None, 55],  # â“ Has missing value
    'salary': [30000, 40000, 50000, 60000, 70000, 80000, 90000],  # ğŸ’°
    'city': ['NY', 'LA', 'NY', 'LA', 'SF', 'SF', 'NY']  # ğŸ™ï¸ Categorical
})

# ğŸ”§ Handle missing values
imputer = SimpleImputer(strategy='mean')  # ğŸ“Š Fill with average
data['age'] = imputer.fit_transform(data[['age']])
print("Missing values filled ğŸ”§ âœ…")

# ğŸ“ Standardization (mean=0, std=1)
scaler = StandardScaler()
data['age_scaled'] = scaler.fit_transform(data[['age']])
data['salary_scaled'] = scaler.fit_transform(data[['salary']])
print("Features standardized ğŸ“ âœ…")
# Good for: Algorithms sensitive to scale (SVM, KNN, Neural Networks)

# ğŸ“Š Normalization (scale to 0-1 range)
normalizer = MinMaxScaler()
data['age_normalized'] = normalizer.fit_transform(data[['age']])
print("Features normalized ğŸ“Š âœ…")

# ğŸ·ï¸ Label Encoding (convert categories to numbers)
label_encoder = LabelEncoder()
data['city_encoded'] = label_encoder.fit_transform(data['city'])
print("Labels encoded ğŸ·ï¸ âœ…")
# NY â†’ 0, LA â†’ 1, SF â†’ 2

# ğŸ”¢ One-Hot Encoding (create binary columns)
data_encoded = pd.get_dummies(data, columns=['city'], prefix='city')
print("One-hot encoding ğŸ”¢ âœ…")
# Creates: city_NY, city_LA, city_SF (1 or 0)

# âœ‚ï¸ Train-Test Split (separate data for training and testing)
X = iris.data  # ğŸ“Š Features
y = iris.target  # ğŸ¯ Labels
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,  # ğŸ“Š 20% for testing, 80% for training
    random_state=42  # ğŸ² Reproducible
)
print(f"Training set ğŸ“š: {X_train.shape}")
print(f"Testing set ğŸ§ª: {X_test.shape}")
```

### ğŸ” Feature Engineering

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.feature_selection import SelectKBest, chi2

# ğŸ”¢ Polynomial Features (create interaction terms)
poly = PolynomialFeatures(degree=2, include_bias=False)
X_sample = np.array([[1, 2], [3, 4]])
X_poly = poly.fit_transform(X_sample)
print("Original features ğŸ“Š:", X_sample.shape)
print("Polynomial features ğŸ”¢:", X_poly.shape)
# Adds: xâ‚, xâ‚‚, xâ‚Â², xâ‚xâ‚‚, xâ‚‚Â²

# ğŸ¯ Feature Selection (keep only best features)
selector = SelectKBest(chi2, k=2)  # ğŸ¯ Select top 2 features
X_selected = selector.fit_transform(iris.data, iris.target)
print("Features selected ğŸ¯:", X_selected.shape)
selected_indices = selector.get_support(indices=True)
print("Best feature indices ğŸ†:", selected_indices)
```

**ğŸ’¡ Linear Explanation:** Sklearn preprocessing prepares raw data for machine learning. We handle missing values, scale features to similar ranges, convert categories to numbers, split data into training/testing sets, and select the most important features.

---

## ğŸ¯ Complete Integration Example ğŸš€

```python
import numpy as np
import pandas as pd
from scipy import stats
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

# ğŸ“¦ Step 1: Load dataset
iris = datasets.load_iris()
data = pd.DataFrame(iris.data, columns=iris.feature_names)
data['species'] = iris.target
print("Dataset loaded ğŸ“¦ âœ…")

# ğŸ”¢ Step 2: NumPy operations (vectorization)
sepal_lengths = data['sepal length (cm)'].values
# âš¡ Fast vectorized calculation
normalized_lengths = (sepal_lengths - np.mean(sepal_lengths)) / np.std(sepal_lengths)
print("NumPy vectorization âš¡ completed")

# ğŸ“Š Step 3: SciPy statistics
mean = np.mean(sepal_lengths)
median = np.median(sepal_lengths)
mode = stats.mode(sepal_lengths)
print(f"Mean ğŸ“ˆ: {mean:.2f}, Median ğŸ“Š: {median:.2f}")

# ğŸ§ª Normality test
statistic, p_value = stats.shapiro(sepal_lengths)
print(f"Normality test ğŸ§ª p-value: {p_value:.4f}")

# ğŸ“ˆ Step 4: Statsmodels regression
X = data[['sepal width (cm)']]
y = data['sepal length (cm)']
X = sm.add_constant(X)
model = sm.OLS(y, X).fit()
print(f"R-squared ğŸ“Š: {model.rsquared:.4f}")

# ğŸ› ï¸ Step 5: Sklearn preprocessing
scaler = StandardScaler()
X_features = data.drop('species', axis=1)
X_scaled = scaler.fit_transform(X_features)
print("Features scaled ğŸ› ï¸ âœ…")

# âœ‚ï¸ Step 6: Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, data['species'], 
    test_size=0.2, 
    random_state=42
)
print(f"Data split âœ‚ï¸: Train={X_train.shape}, Test={X_test.shape}")

print("\nğŸ‰ Complete integration successful! ğŸš€")
```

---

## ğŸ“ What We Learned Today

â€¢ **ğŸ”¢ NumPy Advanced Techniques:** Master broadcasting ğŸ“¡ for operations on different shapes, vectorization âš¡ for 100x speed boost avoiding loops, and matrix operations ğŸ§® for linear algebra (multiplication, inverse, eigenvalues)

â€¢ **ğŸ”¬ SciPy Scientific Computing:** Use statistical functions ğŸ“Š (distributions, t-tests, correlation), optimization ğŸ¯ (find min/max), integration âˆ« (calculate areas), and signal processing ğŸŒŠ for real-world science

â€¢ **ğŸ“ˆ Statsmodels Statistical Modeling:** Build regression models ğŸ“‰ to predict values and understand relationships, perform time series analysis ğŸ“… (decomposition, stationarity tests), and logistic regression ğŸ¯ for classification

â€¢ **ğŸ¤– Sklearn Machine Learning Prep:** Load built-in datasets ğŸ“¦ (Iris ğŸŒ¸, Wine ğŸ·, Digits ğŸ”¢), preprocess data ğŸ› ï¸ (scaling, encoding, imputation), split data âœ‚ï¸ for training/testing, and engineer features ğŸ”§ for better model performance

â€¢ **ğŸ”— Library Integration Power:** Combine NumPy ğŸ”¢, SciPy ğŸ”¬, Statsmodels ğŸ“ˆ, and Sklearn ğŸ¤– in one workflow for complete data science pipelines from raw data to statistical analysis to machine learning preparation ğŸš€